11/05/2025 12:59:14 - INFO - root -   Training args Namespace(output_name='llama2-7b-standard', datasets=['lmsys'], pretrain_name='llama2-7b', loss_weight=1.0, sven=False, num_train_epochs=2, learning_rate=2e-05, max_num_tokens=1024, batch_size=1, grad_acc_steps=16, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, kl_loss_weight=0, exclude_neg=False, no_weights=False, lora=False, r=16, lora_alpha=32, lora_dropout=0.1, sampling_size=40, sampling_method='minority', cwes=['all'], langs=['all'], logging_steps=50, save_epochs=10, seed=2, data_dir='../data_train_val', model_dir='../trained/', output_dir='../trained/llama2-7b-standard', logger=<RootLogger root (INFO)>)
11/05/2025 12:59:14 - INFO - root -   ***** Running training *****
11/05/2025 12:59:14 - INFO - root -     Num samples = 16194
11/05/2025 12:59:14 - INFO - root -     Num epoch = 2
11/05/2025 12:59:14 - INFO - root -     Batch size= 1
11/05/2025 12:59:14 - INFO - root -     Total batch size (w. accumulation) = 16
11/05/2025 12:59:14 - INFO - root -     Gradient Accumulation steps = 16
11/05/2025 12:59:14 - INFO - root -     Total optimization steps = 2024
11/05/2025 12:59:14 - INFO - root -     Num val samples = 1798
11/05/2025 12:59:14 - INFO - root -     Num parameters = 6738415616
11/05/2025 12:59:14 - INFO - root -     Num trainable parameters = 6738415616
11/05/2025 13:11:59 - INFO - root -   epochs: 1/2, steps: 50/2024, func: 0.068502, 2%: 8h 24m 37s          
11/05/2025 13:25:16 - INFO - root -   epochs: 1/2, steps: 100/2024, func: 0.066076, 4%: 8h 20m 36s          
11/05/2025 13:38:12 - INFO - root -   epochs: 1/2, steps: 150/2024, func: 0.06327, 7%: 8h 7m 38s          
11/05/2025 13:51:25 - INFO - root -   epochs: 1/2, steps: 200/2024, func: 0.063638, 9%: 7h 55m 42s          
11/05/2025 14:04:14 - INFO - root -   epochs: 1/2, steps: 250/2024, func: 0.060069, 12%: 7h 41m 39s          
11/05/2025 14:16:40 - INFO - root -   epochs: 1/2, steps: 300/2024, func: 0.060631, 14%: 7h 25m 48s          
11/05/2025 14:29:50 - INFO - root -   epochs: 1/2, steps: 350/2024, func: 0.060688, 17%: 7h 13m 36s          
11/05/2025 14:42:22 - INFO - root -   epochs: 1/2, steps: 400/2024, func: 0.059972, 19%: 6h 58m 49s          
11/05/2025 14:55:02 - INFO - root -   epochs: 1/2, steps: 450/2024, func: 0.060049, 22%: 6h 45m 24s          
11/05/2025 15:07:36 - INFO - root -   epochs: 1/2, steps: 500/2024, func: 0.06059, 24%: 6h 31m 27s          
11/05/2025 15:20:42 - INFO - root -   epochs: 1/2, steps: 550/2024, func: 0.060892, 27%: 6h 19m 21s          
11/05/2025 15:33:31 - INFO - root -   epochs: 1/2, steps: 600/2024, func: 0.058437, 29%: 6h 6m 24s          
11/05/2025 15:46:02 - INFO - root -   epochs: 1/2, steps: 650/2024, func: 0.05827, 32%: 5h 52m 56s          
11/05/2025 15:58:54 - INFO - root -   epochs: 1/2, steps: 700/2024, func: 0.058516, 34%: 5h 40m 6s          
11/05/2025 16:11:58 - INFO - root -   epochs: 1/2, steps: 750/2024, func: 0.058642, 37%: 5h 27m 38s          
11/05/2025 16:24:39 - INFO - root -   epochs: 1/2, steps: 800/2024, func: 0.058784, 39%: 5h 14m 35s          
11/05/2025 16:37:12 - INFO - root -   epochs: 1/2, steps: 850/2024, func: 0.056264, 41%: 5h 1m 20s          
11/05/2025 16:49:58 - INFO - root -   epochs: 1/2, steps: 900/2024, func: 0.057284, 44%: 4h 48m 22s          
11/05/2025 17:02:26 - INFO - root -   epochs: 1/2, steps: 950/2024, func: 0.056305, 46%: 4h 35m 12s          
11/05/2025 17:15:21 - INFO - root -   epochs: 1/2, steps: 1000/2024, func: 0.054719, 49%: 4h 22m 31s          
11/05/2025 17:28:36 - INFO - root -   epochs: 2/2, steps: 1050/2024, func: 0.041421, 51%: 4h 10m 6s          
11/05/2025 17:41:19 - INFO - root -   epochs: 2/2, steps: 1100/2024, func: 0.034935, 54%: 3h 57m 13s          
11/05/2025 17:54:12 - INFO - root -   epochs: 2/2, steps: 1150/2024, func: 0.03676, 56%: 3h 44m 26s          
11/05/2025 18:06:55 - INFO - root -   epochs: 2/2, steps: 1200/2024, func: 0.03459, 59%: 3h 31m 30s          
11/05/2025 18:19:54 - INFO - root -   epochs: 2/2, steps: 1250/2024, func: 0.036715, 61%: 3h 18m 48s          
11/05/2025 18:32:52 - INFO - root -   epochs: 2/2, steps: 1300/2024, func: 0.036086, 64%: 3h 6m 5s          
11/05/2025 18:45:50 - INFO - root -   epochs: 2/2, steps: 1350/2024, func: 0.037209, 66%: 2h 53m 18s          
11/05/2025 18:58:17 - INFO - root -   epochs: 2/2, steps: 1400/2024, func: 0.03664, 69%: 2h 40m 18s          
11/05/2025 19:11:13 - INFO - root -   epochs: 2/2, steps: 1450/2024, func: 0.035179, 71%: 2h 27m 30s          
11/05/2025 19:23:38 - INFO - root -   epochs: 2/2, steps: 1500/2024, func: 0.035552, 74%: 2h 14m 32s          
11/05/2025 19:36:04 - INFO - root -   epochs: 2/2, steps: 1550/2024, func: 0.034601, 76%: 2h 1m 36s          
11/05/2025 19:48:55 - INFO - root -   epochs: 2/2, steps: 1600/2024, func: 0.037225, 79%: 1h 48m 49s          
11/05/2025 20:01:45 - INFO - root -   epochs: 2/2, steps: 1650/2024, func: 0.034085, 81%: 1h 36m 0s          
11/05/2025 20:14:50 - INFO - root -   epochs: 2/2, steps: 1700/2024, func: 0.033972, 83%: 1h 23m 16s          
11/05/2025 20:27:29 - INFO - root -   epochs: 2/2, steps: 1750/2024, func: 0.034746, 86%: 1h 10m 26s          
11/05/2025 20:40:15 - INFO - root -   epochs: 2/2, steps: 1800/2024, func: 0.035967, 88%: 0h 57m 37s          
11/05/2025 20:53:00 - INFO - root -   epochs: 2/2, steps: 1850/2024, func: 0.03514, 91%: 0h 44m 49s          
11/05/2025 21:05:35 - INFO - root -   epochs: 2/2, steps: 1900/2024, func: 0.03293, 93%: 0h 31m 59s          
11/05/2025 21:18:35 - INFO - root -   epochs: 2/2, steps: 1950/2024, func: 0.032303, 96%: 0h 19m 12s          
11/05/2025 21:31:25 - INFO - root -   epochs: 2/2, steps: 2000/2024, func: 0.033693, 98%: 0h 6m 24s          
11/05/2025 21:46:37 - INFO - root -   final eval loss: func: 0.053657
11/05/2025 21:46:37 - INFO - root -   Saving model checkpoint to ../trained/llama2-7b-standard/checkpoint-last
