11/04/2025 13:36:35 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
11/04/2025 13:36:50 - INFO - root -   Training args Namespace(output_name='phi-2-standard', datasets=['lmsys'], pretrain_name='phi-2', loss_weight=1.0, sven=False, num_train_epochs=2, learning_rate=2e-05, max_num_tokens=1024, batch_size=1, grad_acc_steps=16, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, kl_loss_weight=0, exclude_neg=False, no_weights=False, lora=False, r=16, lora_alpha=32, lora_dropout=0.1, sampling_size=40, sampling_method='minority', cwes=['all'], langs=['all'], logging_steps=50, save_epochs=10, seed=2, data_dir='../data_train_val', model_dir='../trained/', output_dir='../trained/phi-2-standard', logger=<RootLogger root (INFO)>)
11/04/2025 13:36:50 - INFO - root -   ***** Running training *****
11/04/2025 13:36:50 - INFO - root -     Num samples = 16282
11/04/2025 13:36:50 - INFO - root -     Num epoch = 2
11/04/2025 13:36:50 - INFO - root -     Batch size= 1
11/04/2025 13:36:50 - INFO - root -     Total batch size (w. accumulation) = 16
11/04/2025 13:36:50 - INFO - root -     Gradient Accumulation steps = 16
11/04/2025 13:36:50 - INFO - root -     Total optimization steps = 2034
11/04/2025 13:36:50 - INFO - root -     Num val samples = 1811
11/04/2025 13:36:50 - INFO - root -     Num parameters = 2775049335
11/04/2025 13:36:50 - INFO - root -     Num trainable parameters = 2775049335
11/04/2025 13:42:24 - INFO - root -   epochs: 1/2, steps: 50/2034, func: 0.078043, 2%: 3h 41m 2s          
11/04/2025 13:47:58 - INFO - root -   epochs: 1/2, steps: 100/2034, func: 0.06971, 4%: 3h 35m 50s          
11/04/2025 13:53:30 - INFO - root -   epochs: 1/2, steps: 150/2034, func: 0.068909, 7%: 3h 29m 6s          
11/04/2025 13:59:10 - INFO - root -   epochs: 1/2, steps: 200/2034, func: 0.069741, 9%: 3h 24m 59s          
11/04/2025 14:04:44 - INFO - root -   epochs: 1/2, steps: 250/2034, func: 0.070073, 12%: 3h 19m 20s          
11/04/2025 14:10:12 - INFO - root -   epochs: 1/2, steps: 300/2034, func: 0.068079, 14%: 3h 13m 9s          
11/04/2025 14:15:41 - INFO - root -   epochs: 1/2, steps: 350/2034, func: 0.065383, 17%: 3h 6m 52s          
11/04/2025 14:21:09 - INFO - root -   epochs: 1/2, steps: 400/2034, func: 0.064645, 19%: 3h 1m 6s          
11/04/2025 14:26:41 - INFO - root -   epochs: 1/2, steps: 450/2034, func: 0.067363, 22%: 2h 55m 28s          
11/04/2025 14:32:14 - INFO - root -   epochs: 1/2, steps: 500/2034, func: 0.066711, 24%: 2h 50m 2s          
11/04/2025 14:37:43 - INFO - root -   epochs: 1/2, steps: 550/2034, func: 0.064435, 26%: 2h 44m 26s          
11/04/2025 14:43:12 - INFO - root -   epochs: 1/2, steps: 600/2034, func: 0.063601, 29%: 2h 38m 43s          
11/04/2025 14:48:44 - INFO - root -   epochs: 1/2, steps: 650/2034, func: 0.063504, 31%: 2h 33m 11s          
11/04/2025 14:54:15 - INFO - root -   epochs: 1/2, steps: 700/2034, func: 0.06551, 34%: 2h 27m 38s          
11/04/2025 14:59:51 - INFO - root -   epochs: 1/2, steps: 750/2034, func: 0.064414, 36%: 2h 22m 11s          
11/04/2025 15:05:26 - INFO - root -   epochs: 1/2, steps: 800/2034, func: 0.063876, 39%: 2h 16m 45s          
11/04/2025 15:11:05 - INFO - root -   epochs: 1/2, steps: 850/2034, func: 0.063273, 41%: 2h 11m 24s          
11/04/2025 15:16:39 - INFO - root -   epochs: 1/2, steps: 900/2034, func: 0.062534, 44%: 2h 5m 53s          
11/04/2025 15:22:21 - INFO - root -   epochs: 1/2, steps: 950/2034, func: 0.064161, 46%: 2h 0m 30s          
11/04/2025 15:27:53 - INFO - root -   epochs: 1/2, steps: 1000/2034, func: 0.064456, 49%: 1h 54m 56s          
11/04/2025 15:33:14 - INFO - root -   epochs: 2/2, steps: 1050/2034, func: 0.049895, 51%: 1h 49m 13s          
11/04/2025 15:38:49 - INFO - root -   epochs: 2/2, steps: 1100/2034, func: 0.046317, 54%: 1h 43m 40s          
11/04/2025 15:44:28 - INFO - root -   epochs: 2/2, steps: 1150/2034, func: 0.046133, 56%: 1h 38m 13s          
11/04/2025 15:49:53 - INFO - root -   epochs: 2/2, steps: 1200/2034, func: 0.044547, 58%: 1h 32m 35s          
11/04/2025 15:55:22 - INFO - root -   epochs: 2/2, steps: 1250/2034, func: 0.044806, 61%: 1h 27m 0s          
11/04/2025 16:01:00 - INFO - root -   epochs: 2/2, steps: 1300/2034, func: 0.045586, 63%: 1h 21m 30s          
11/04/2025 16:06:39 - INFO - root -   epochs: 2/2, steps: 1350/2034, func: 0.045192, 66%: 1h 16m 0s          
11/04/2025 16:12:06 - INFO - root -   epochs: 2/2, steps: 1400/2034, func: 0.043605, 68%: 1h 10m 25s          
11/04/2025 16:17:40 - INFO - root -   epochs: 2/2, steps: 1450/2034, func: 0.045949, 71%: 1h 4m 53s          
11/04/2025 16:23:10 - INFO - root -   epochs: 2/2, steps: 1500/2034, func: 0.045495, 73%: 0h 59m 19s          
11/04/2025 16:28:43 - INFO - root -   epochs: 2/2, steps: 1550/2034, func: 0.044555, 76%: 0h 53m 47s          
11/04/2025 16:34:13 - INFO - root -   epochs: 2/2, steps: 1600/2034, func: 0.045431, 78%: 0h 48m 13s          
11/04/2025 16:39:52 - INFO - root -   epochs: 2/2, steps: 1650/2034, func: 0.046542, 81%: 0h 42m 42s          
11/04/2025 16:45:30 - INFO - root -   epochs: 2/2, steps: 1700/2034, func: 0.046011, 83%: 0h 37m 10s          
11/04/2025 16:51:00 - INFO - root -   epochs: 2/2, steps: 1750/2034, func: 0.043431, 85%: 0h 31m 37s          
11/04/2025 16:56:27 - INFO - root -   epochs: 2/2, steps: 1800/2034, func: 0.043384, 88%: 0h 26m 3s          
11/04/2025 17:02:01 - INFO - root -   epochs: 2/2, steps: 1850/2034, func: 0.045213, 90%: 0h 20m 31s          
11/04/2025 17:07:41 - INFO - root -   epochs: 2/2, steps: 1900/2034, func: 0.045061, 93%: 0h 14m 59s          
11/04/2025 17:13:19 - INFO - root -   epochs: 2/2, steps: 1950/2034, func: 0.044977, 95%: 0h 9m 26s          
11/04/2025 17:18:52 - INFO - root -   epochs: 2/2, steps: 2000/2034, func: 0.046496, 98%: 0h 3m 53s          
11/04/2025 17:26:52 - INFO - root -   final eval loss: func: 0.059858
11/04/2025 17:26:52 - INFO - root -   Saving model checkpoint to ../trained/phi-2-standard/checkpoint-last
