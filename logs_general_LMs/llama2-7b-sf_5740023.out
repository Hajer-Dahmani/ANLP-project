11/05/2025 12:58:48 - INFO - root -   number of sec samples before upsampling: 1806
11/05/2025 12:58:48 - INFO - root -   number of sec samples after upsampling: 3305
11/05/2025 12:58:51 - INFO - root -   Training args Namespace(output_name='llama2-7b-safecoder', datasets=['lmsys', 'sec-desc', 'sec-new-desc'], pretrain_name='llama2-7b', loss_weight=1.0, sven=False, num_train_epochs=2, learning_rate=2e-05, max_num_tokens=1024, batch_size=1, grad_acc_steps=16, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, kl_loss_weight=0, exclude_neg=False, no_weights=False, lora=False, r=16, lora_alpha=32, lora_dropout=0.1, sampling_size=40, sampling_method='minority', cwes=['all'], langs=['all'], logging_steps=50, save_epochs=10, seed=2, data_dir='../data_train_val', model_dir='../trained/', output_dir='../trained/llama2-7b-safecoder', logger=<RootLogger root (INFO)>)
11/05/2025 12:58:51 - INFO - root -   ***** Running training *****
11/05/2025 12:58:51 - INFO - root -     Num samples = 19499
11/05/2025 12:58:51 - INFO - root -     Num epoch = 2
11/05/2025 12:58:51 - INFO - root -     Batch size= 1
11/05/2025 12:58:51 - INFO - root -     Total batch size (w. accumulation) = 16
11/05/2025 12:58:51 - INFO - root -     Gradient Accumulation steps = 16
11/05/2025 12:58:51 - INFO - root -     Total optimization steps = 2436
11/05/2025 12:58:51 - INFO - root -     Num val samples = 1994
11/05/2025 12:58:51 - INFO - root -     Num parameters = 6738415616
11/05/2025 12:58:51 - INFO - root -     Num trainable parameters = 6738415616
11/05/2025 13:11:27 - INFO - root -   epochs: 1/2, steps: 50/2436, func: 0.067667, pos: 0.093308, neg: 0.094738, 2%: 10h 3m 8s          
11/05/2025 13:23:53 - INFO - root -   epochs: 1/2, steps: 100/2436, func: 0.066805, pos: 0.124739, neg: 0.033545, 4%: 9h 45m 52s          
11/05/2025 13:35:54 - INFO - root -   epochs: 1/2, steps: 150/2436, func: 0.065313, pos: 0.107255, neg: 0.035646, 6%: 9h 25m 10s          
11/05/2025 13:48:41 - INFO - root -   epochs: 1/2, steps: 200/2436, func: 0.065722, pos: 0.070071, neg: 0.021512, 8%: 9h 17m 11s          
11/05/2025 14:01:18 - INFO - root -   epochs: 1/2, steps: 250/2436, func: 0.065312, pos: 0.100842, neg: 0.021053, 10%: 9h 5m 59s          
11/05/2025 14:13:44 - INFO - root -   epochs: 1/2, steps: 300/2436, func: 0.064661, pos: 0.081379, neg: 0.033078, 12%: 8h 53m 30s          
11/05/2025 14:26:02 - INFO - root -   epochs: 1/2, steps: 350/2436, func: 0.064785, pos: 0.104351, neg: 0.017947, 14%: 8h 39m 43s          
11/05/2025 14:38:24 - INFO - root -   epochs: 1/2, steps: 400/2436, func: 0.063342, pos: 0.084776, neg: 0.017507, 16%: 8h 26m 45s          
11/05/2025 14:50:40 - INFO - root -   epochs: 1/2, steps: 450/2436, func: 0.064455, pos: 0.068877, neg: 0.017575, 18%: 8h 13m 46s          
11/05/2025 15:03:03 - INFO - root -   epochs: 1/2, steps: 500/2436, func: 0.06377, pos: 0.080543, neg: 0.024994, 20%: 8h 1m 6s          
11/05/2025 15:15:24 - INFO - root -   epochs: 1/2, steps: 550/2436, func: 0.062476, pos: 0.06877, neg: 0.017903, 22%: 7h 48m 39s          
11/05/2025 15:28:10 - INFO - root -   epochs: 1/2, steps: 600/2436, func: 0.061049, pos: 0.072418, neg: 0.018375, 24%: 7h 37m 3s          
11/05/2025 15:40:35 - INFO - root -   epochs: 1/2, steps: 650/2436, func: 0.059571, pos: 0.06153, neg: 0.019612, 26%: 7h 24m 44s          
11/05/2025 15:53:02 - INFO - root -   epochs: 1/2, steps: 700/2436, func: 0.060961, pos: 0.083072, neg: 0.028486, 28%: 7h 12m 6s          
11/05/2025 16:05:39 - INFO - root -   epochs: 1/2, steps: 750/2436, func: 0.060067, pos: 0.04825, neg: 0.024249, 30%: 7h 0m 14s          
11/05/2025 16:18:09 - INFO - root -   epochs: 1/2, steps: 800/2436, func: 0.058932, pos: 0.064833, neg: 0.019097, 32%: 6h 47m 54s          
11/05/2025 16:30:50 - INFO - root -   epochs: 1/2, steps: 850/2436, func: 0.059335, pos: 0.058667, neg: 0.008357, 34%: 6h 35m 48s          
11/05/2025 16:43:02 - INFO - root -   epochs: 1/2, steps: 900/2436, func: 0.057952, pos: 0.069313, neg: 0.018689, 36%: 6h 22m 54s          
11/05/2025 16:55:10 - INFO - root -   epochs: 1/2, steps: 950/2436, func: 0.059025, pos: 0.065731, neg: 0.011373, 38%: 6h 9m 51s          
11/05/2025 17:07:25 - INFO - root -   epochs: 1/2, steps: 1000/2436, func: 0.057681, pos: 0.046972, neg: 0.013166, 41%: 5h 57m 11s          
11/05/2025 17:19:58 - INFO - root -   epochs: 1/2, steps: 1050/2436, func: 0.058866, pos: 0.070605, neg: 0.016933, 43%: 5h 44m 57s          
11/05/2025 17:32:18 - INFO - root -   epochs: 1/2, steps: 1100/2436, func: 0.057832, pos: 0.050154, neg: 0.007894, 45%: 5h 32m 19s          
11/05/2025 17:44:25 - INFO - root -   epochs: 1/2, steps: 1150/2436, func: 0.058733, pos: 0.04017, neg: 0.009375, 47%: 5h 19m 37s          
11/05/2025 17:56:47 - INFO - root -   epochs: 1/2, steps: 1200/2436, func: 0.056801, pos: 0.044091, neg: 0.019236, 49%: 5h 7m 4s          
11/05/2025 18:09:45 - INFO - root -   epochs: 2/2, steps: 1250/2436, func: 0.047393, pos: 0.045274, neg: 0.011565, 51%: 4h 55m 11s          
11/05/2025 18:22:09 - INFO - root -   epochs: 2/2, steps: 1300/2436, func: 0.039483, pos: 0.036758, neg: 0.007215, 53%: 4h 42m 46s          
11/05/2025 18:34:48 - INFO - root -   epochs: 2/2, steps: 1350/2436, func: 0.037931, pos: 0.01762, neg: 0.008198, 55%: 4h 30m 29s          
11/05/2025 18:47:21 - INFO - root -   epochs: 2/2, steps: 1400/2436, func: 0.038531, pos: 0.030466, neg: 0.010711, 57%: 4h 18m 9s          
11/05/2025 18:59:29 - INFO - root -   epochs: 2/2, steps: 1450/2436, func: 0.039343, pos: 0.070642, neg: 0.008031, 59%: 4h 5m 26s          
11/05/2025 19:11:58 - INFO - root -   epochs: 2/2, steps: 1500/2436, func: 0.039824, pos: 0.037383, neg: 0.009172, 61%: 3h 53m 4s          
11/05/2025 19:24:21 - INFO - root -   epochs: 2/2, steps: 1550/2436, func: 0.040031, pos: 0.033753, neg: 0.008633, 63%: 3h 40m 35s          
11/05/2025 19:36:50 - INFO - root -   epochs: 2/2, steps: 1600/2436, func: 0.03985, pos: 0.02461, neg: 0.008631, 65%: 3h 28m 11s          
11/05/2025 19:48:58 - INFO - root -   epochs: 2/2, steps: 1650/2436, func: 0.040072, pos: 0.016264, neg: 0.013597, 67%: 3h 15m 34s          
11/05/2025 20:01:39 - INFO - root -   epochs: 2/2, steps: 1700/2436, func: 0.038609, pos: 0.026833, neg: 0.008521, 69%: 3h 3m 17s          
11/05/2025 20:13:21 - INFO - root -   epochs: 2/2, steps: 1750/2436, func: 0.036685, pos: 0.035755, neg: 0.008312, 71%: 2h 50m 33s          
11/05/2025 20:25:53 - INFO - root -   epochs: 2/2, steps: 1800/2436, func: 0.039227, pos: 0.030342, neg: 0.016396, 73%: 2h 38m 11s          
11/05/2025 20:38:10 - INFO - root -   epochs: 2/2, steps: 1850/2436, func: 0.037578, pos: 0.018274, neg: 0.003642, 75%: 2h 25m 44s          
11/05/2025 20:50:24 - INFO - root -   epochs: 2/2, steps: 1900/2436, func: 0.03824, pos: 0.022091, neg: 0.004798, 77%: 2h 13m 16s          
11/05/2025 21:02:58 - INFO - root -   epochs: 2/2, steps: 1950/2436, func: 0.038192, pos: 0.031005, neg: 0.0107, 80%: 2h 0m 54s          
11/05/2025 21:15:30 - INFO - root -   epochs: 2/2, steps: 2000/2436, func: 0.036612, pos: 0.021469, neg: 0.008719, 82%: 1h 48m 31s          
11/05/2025 21:27:46 - INFO - root -   epochs: 2/2, steps: 2050/2436, func: 0.037318, pos: 0.033304, neg: 0.008844, 84%: 1h 36m 4s          
11/05/2025 21:40:27 - INFO - root -   epochs: 2/2, steps: 2100/2436, func: 0.036342, pos: 0.029397, neg: 0.008638, 86%: 1h 23m 42s          
11/05/2025 21:52:52 - INFO - root -   epochs: 2/2, steps: 2150/2436, func: 0.036837, pos: 0.026468, neg: 0.011145, 88%: 1h 11m 16s          
11/05/2025 22:05:26 - INFO - root -   epochs: 2/2, steps: 2200/2436, func: 0.035176, pos: 0.025759, neg: 0.012045, 90%: 0h 58m 52s          
11/05/2025 22:17:48 - INFO - root -   epochs: 2/2, steps: 2250/2436, func: 0.036444, pos: 0.027725, neg: 0.005209, 92%: 0h 46m 27s          
11/05/2025 22:29:58 - INFO - root -   epochs: 2/2, steps: 2300/2436, func: 0.035807, pos: 0.015975, neg: 0.003526, 94%: 0h 34m 0s          
11/05/2025 22:42:25 - INFO - root -   epochs: 2/2, steps: 2350/2436, func: 0.035983, pos: 0.027896, neg: 0.011744, 96%: 0h 21m 36s          
11/05/2025 22:54:58 - INFO - root -   epochs: 2/2, steps: 2400/2436, func: 0.037792, pos: 0.026865, neg: 0.006981, 98%: 0h 9m 11s          
11/05/2025 23:14:14 - INFO - root -   final eval loss: func: 0.053648, pos: 0.061082, neg: 0.023588
11/05/2025 23:14:14 - INFO - root -   Saving model checkpoint to ../trained/llama2-7b-safecoder/checkpoint-last
