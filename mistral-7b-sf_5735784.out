11/04/2025 18:04:17 - INFO - root -   number of sec samples before upsampling: 1809
11/04/2025 18:04:17 - INFO - root -   number of sec samples after upsampling: 3347
11/04/2025 18:04:19 - INFO - root -   Training args Namespace(output_name='mistral-7b-safecoder', datasets=['lmsys', 'sec-desc', 'sec-new-desc'], pretrain_name='mistral-7b', loss_weight=1.0, sven=False, num_train_epochs=2, learning_rate=2e-05, max_num_tokens=1024, batch_size=1, grad_acc_steps=16, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, kl_loss_weight=0, exclude_neg=False, no_weights=False, lora=False, r=16, lora_alpha=32, lora_dropout=0.1, sampling_size=40, sampling_method='minority', cwes=['all'], langs=['all'], logging_steps=50, save_epochs=10, seed=2, data_dir='../data_train_val', model_dir='../trained/', output_dir='../trained/mistral-7b-safecoder', logger=<RootLogger root (INFO)>)
11/04/2025 18:04:19 - INFO - root -   ***** Running training *****
11/04/2025 18:04:19 - INFO - root -     Num samples = 19568
11/04/2025 18:04:19 - INFO - root -     Num epoch = 2
11/04/2025 18:04:19 - INFO - root -     Batch size= 1
11/04/2025 18:04:19 - INFO - root -     Total batch size (w. accumulation) = 16
11/04/2025 18:04:19 - INFO - root -     Gradient Accumulation steps = 16
11/04/2025 18:04:19 - INFO - root -     Total optimization steps = 2446
11/04/2025 18:04:19 - INFO - root -     Num val samples = 1999
11/04/2025 18:04:19 - INFO - root -     Num parameters = 7241732096
11/04/2025 18:04:19 - INFO - root -     Num trainable parameters = 7241732096
11/04/2025 18:16:38 - INFO - root -   epochs: 1/2, steps: 50/2446, func: 0.436968, pos: 0.49664, neg: 0.013439, 2%: 9h 51m 0s          
11/04/2025 18:29:00 - INFO - root -   epochs: 1/2, steps: 100/2446, func: 0.450991, pos: 0.488955, neg: 0.000302, 4%: 9h 39m 29s          
11/04/2025 18:41:22 - INFO - root -   epochs: 1/2, steps: 150/2446, func: 0.425557, pos: 0.482158, neg: 0.000967, 6%: 9h 27m 33s          
11/04/2025 18:54:05 - INFO - root -   epochs: 1/2, steps: 200/2446, func: 0.412706, pos: 0.457854, neg: 0.000816, 8%: 9h 19m 37s          
11/04/2025 19:06:25 - INFO - root -   epochs: 1/2, steps: 250/2446, func: 0.393196, pos: 0.439533, neg: 0.000692, 10%: 9h 5m 31s          
11/04/2025 19:18:49 - INFO - root -   epochs: 1/2, steps: 300/2446, func: 0.386982, pos: 0.422873, neg: 0.001346, 12%: 8h 53m 8s          
11/04/2025 19:31:13 - INFO - root -   epochs: 1/2, steps: 350/2446, func: 0.379908, pos: 0.408609, neg: 0.001016, 14%: 8h 40m 42s          
11/04/2025 19:43:27 - INFO - root -   epochs: 1/2, steps: 400/2446, func: 0.368207, pos: 0.396182, neg: 0.001582, 16%: 8h 27m 14s          
11/04/2025 19:55:39 - INFO - root -   epochs: 1/2, steps: 450/2446, func: 0.361654, pos: 0.396172, neg: 0.001135, 18%: 8h 13m 42s          
11/04/2025 20:07:57 - INFO - root -   epochs: 1/2, steps: 500/2446, func: 0.355425, pos: 0.398602, neg: 0.00121, 20%: 8h 1m 27s          
11/04/2025 20:20:06 - INFO - root -   epochs: 1/2, steps: 550/2446, func: 0.345546, pos: 0.390691, neg: 0.001596, 22%: 7h 48m 15s          
11/04/2025 20:32:26 - INFO - root -   epochs: 1/2, steps: 600/2446, func: 0.339409, pos: 0.358505, neg: 0.0013, 24%: 7h 36m 2s          
11/04/2025 20:44:57 - INFO - root -   epochs: 1/2, steps: 650/2446, func: 0.334633, pos: 0.374037, neg: 0.00108, 26%: 7h 24m 4s          
11/04/2025 20:56:46 - INFO - root -   epochs: 1/2, steps: 700/2446, func: 0.329126, pos: 0.36283, neg: 0.001346, 28%: 7h 10m 26s          
11/04/2025 21:09:35 - INFO - root -   epochs: 1/2, steps: 750/2446, func: 0.329657, pos: 0.349549, neg: 0.001312, 30%: 6h 59m 16s          
11/04/2025 21:21:32 - INFO - root -   epochs: 1/2, steps: 800/2446, func: 0.316275, pos: 0.348767, neg: 0.001522, 32%: 6h 46m 0s          
11/04/2025 21:34:11 - INFO - root -   epochs: 1/2, steps: 850/2446, func: 0.315862, pos: 0.315468, neg: 0.001616, 34%: 6h 34m 21s          
11/04/2025 21:47:01 - INFO - root -   epochs: 1/2, steps: 900/2446, func: 0.314128, pos: 0.36058, neg: 0.00142, 36%: 6h 22m 51s          
11/04/2025 21:59:30 - INFO - root -   epochs: 1/2, steps: 950/2446, func: 0.306961, pos: 0.32431, neg: 0.001757, 38%: 6h 10m 36s          
11/04/2025 22:11:49 - INFO - root -   epochs: 1/2, steps: 1000/2446, func: 0.305348, pos: 0.323386, neg: 0.001335, 40%: 5h 58m 11s          
11/04/2025 22:24:08 - INFO - root -   epochs: 1/2, steps: 1050/2446, func: 0.308075, pos: 0.333325, neg: 0.002421, 42%: 5h 45m 43s          
11/04/2025 22:36:26 - INFO - root -   epochs: 1/2, steps: 1100/2446, func: 0.30221, pos: 0.317817, neg: 0.00202, 44%: 5h 33m 12s          
11/04/2025 22:48:50 - INFO - root -   epochs: 1/2, steps: 1150/2446, func: 0.303036, pos: 0.314395, neg: 0.00179, 46%: 5h 20m 52s          
11/04/2025 23:00:58 - INFO - root -   epochs: 1/2, steps: 1200/2446, func: 0.300713, pos: 0.317943, neg: 0.002259, 49%: 5h 8m 18s          
11/04/2025 23:13:21 - INFO - root -   epochs: 2/2, steps: 1250/2446, func: 0.295104, pos: 0.296493, neg: 0.002, 51%: 4h 55m 55s          
11/04/2025 23:25:32 - INFO - root -   epochs: 2/2, steps: 1300/2446, func: 0.285965, pos: 0.332857, neg: 0.002093, 53%: 4h 43m 26s          
11/04/2025 23:37:49 - INFO - root -   epochs: 2/2, steps: 1350/2446, func: 0.288422, pos: 0.29718, neg: 0.001676, 55%: 4h 30m 59s          
11/04/2025 23:50:12 - INFO - root -   epochs: 2/2, steps: 1400/2446, func: 0.284182, pos: 0.300995, neg: 0.002375, 57%: 4h 18m 38s          
11/05/2025 00:02:40 - INFO - root -   epochs: 2/2, steps: 1450/2446, func: 0.282408, pos: 0.312871, neg: 0.00201, 59%: 4h 6m 23s          
11/05/2025 00:15:15 - INFO - root -   epochs: 2/2, steps: 1500/2446, func: 0.286062, pos: 0.290618, neg: 0.001974, 61%: 3h 54m 10s          
11/05/2025 00:27:25 - INFO - root -   epochs: 2/2, steps: 1550/2446, func: 0.278584, pos: 0.293783, neg: 0.002095, 63%: 3h 41m 42s          
11/05/2025 00:39:51 - INFO - root -   epochs: 2/2, steps: 1600/2446, func: 0.284944, pos: 0.278218, neg: 0.001598, 65%: 3h 29m 24s          
11/05/2025 00:52:17 - INFO - root -   epochs: 2/2, steps: 1650/2446, func: 0.279286, pos: 0.261566, neg: 0.001938, 67%: 3h 17m 2s          
11/05/2025 01:04:22 - INFO - root -   epochs: 2/2, steps: 1700/2446, func: 0.271271, pos: 0.282575, neg: 0.002282, 69%: 3h 4m 35s          
11/05/2025 01:16:47 - INFO - root -   epochs: 2/2, steps: 1750/2446, func: 0.268197, pos: 0.26467, neg: 0.002323, 71%: 2h 52m 15s          
11/05/2025 01:29:19 - INFO - root -   epochs: 2/2, steps: 1800/2446, func: 0.264264, pos: 0.283959, neg: 0.001655, 73%: 2h 39m 57s          
11/05/2025 01:41:45 - INFO - root -   epochs: 2/2, steps: 1850/2446, func: 0.274283, pos: 0.273818, neg: 0.002762, 75%: 2h 27m 35s          
11/05/2025 01:54:10 - INFO - root -   epochs: 2/2, steps: 1900/2446, func: 0.262271, pos: 0.247547, neg: 0.001766, 77%: 2h 15m 15s          
11/05/2025 02:06:18 - INFO - root -   epochs: 2/2, steps: 1950/2446, func: 0.264213, pos: 0.267811, neg: 0.002174, 79%: 2h 2m 50s          
11/05/2025 02:18:42 - INFO - root -   epochs: 2/2, steps: 2000/2446, func: 0.261688, pos: 0.256338, neg: 0.002066, 81%: 1h 50m 29s          
11/05/2025 02:30:36 - INFO - root -   epochs: 2/2, steps: 2050/2446, func: 0.25666, pos: 0.268575, neg: 0.002527, 83%: 1h 38m 2s          
11/05/2025 02:43:00 - INFO - root -   epochs: 2/2, steps: 2100/2446, func: 0.266579, pos: 0.229054, neg: 0.001786, 85%: 1h 25m 42s          
11/05/2025 02:55:20 - INFO - root -   epochs: 2/2, steps: 2150/2446, func: 0.248532, pos: 0.265689, neg: 0.002113, 87%: 1h 13m 21s          
11/05/2025 03:07:43 - INFO - root -   epochs: 2/2, steps: 2200/2446, func: 0.257778, pos: 0.235846, neg: 0.002103, 89%: 1h 1m 0s          
11/05/2025 03:19:54 - INFO - root -   epochs: 2/2, steps: 2250/2446, func: 0.258755, pos: 0.248866, neg: 0.00397, 91%: 0h 48m 38s          
11/05/2025 03:32:09 - INFO - root -   epochs: 2/2, steps: 2300/2446, func: 0.260427, pos: 0.246518, neg: 0.002185, 93%: 0h 36m 17s          
11/05/2025 03:44:58 - INFO - root -   epochs: 2/2, steps: 2350/2446, func: 0.251513, pos: 0.237565, neg: 0.00309, 96%: 0h 23m 57s          
11/05/2025 03:57:14 - INFO - root -   epochs: 2/2, steps: 2400/2446, func: 0.252342, pos: 0.217567, neg: 0.002209, 98%: 0h 11m 36s          
11/05/2025 04:18:18 - INFO - root -   final eval loss: func: 0.251023, pos: 0.284077, neg: 0.003351
11/05/2025 04:18:18 - INFO - root -   Saving model checkpoint to ../trained/mistral-7b-safecoder/checkpoint-last
