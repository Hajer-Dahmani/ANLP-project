### **Baseline Reproduction Summary**

We successfully reproduced the **SafeCoder** baseline by strictly following the official instructions and training scripts provided in the [SafeCoder GitHub repository](https://github.com/eth-sri/SafeCoder/tree/main). Our reproduction pipeline mirrors as close as possible the setup described in the original paper, including dataset preprocessing, training hyperparameters, and evaluation methodology.

We include all our training logs in this directory.

### **Model Artifacts**

Due to storage limitations on GitHub, we cannot host the full set of reproduced model checkpoints here. Instead, we provide Google Drive links to two representative SafeCoder-tuned models:

* **One finetuned coding language model** trained using SafeCoderâ€™s instruction-tuning framework.
* **One finetuned general-purpose language model** trained with the same methodology.

They are available [here](https://drive.google.com/drive/folders/1EgLA6UtEhyajFBWfZd8WWjLuuqkYo4nY)
